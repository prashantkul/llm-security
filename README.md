# llm-security
This repository contains the code to test various security issues with LLM prompts

## Features
- Google Cloud Model Armor for a comprehensive prompt attacks.
- Huggingface DeBerta model for prompt injection detection.

## Models
- Huggingface
- Google Gemini
- OpenAI GPT
- Anthropic Claude

## Requirements
- Need a Google Cloud project that is enabled for Model Armor
- Store API keys as a secrets in Google Cloud Secrets Manager

## Output
- Provides a command line output for various prompt detection
- Generates a CSV file with summary.


